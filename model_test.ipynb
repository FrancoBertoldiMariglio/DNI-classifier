{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Test",
   "id": "7c1f2942b2813a97"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "from ultralytics import YOLO\n",
    "from autoencoder import DNIAnomalyDetector, DNIDataset\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_reconstruction_errors(detector, data_dir, batch_size=32):\n",
    "    \"\"\"Helper function to compute reconstruction errors for a directory.\"\"\"\n",
    "    dataset = DNIDataset(data_dir, detector.transform)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    reconstruction_errors = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc=f\"Processing {data_dir}\"):\n",
    "            imgs = batch.to(detector.device)\n",
    "            reconstructed = detector.decoder(detector.encoder(imgs))\n",
    "            \n",
    "            errors = torch.nn.functional.mse_loss(\n",
    "                reconstructed, imgs, reduction='none'\n",
    "            ).mean(dim=[1,2,3]).cpu().numpy()\n",
    "            \n",
    "            reconstruction_errors.extend(errors)\n",
    "            \n",
    "    return np.array(reconstruction_errors)\n",
    "\n",
    "def preprocess_dataset_with_yolo(yolo_model, input_dir, output_dir, invalid_labels=None):\n",
    "    \"\"\"\n",
    "    Preprocesa un directorio de imágenes usando YOLO para detectar y recortar DNIs.\n",
    "    \n",
    "    Args:\n",
    "        yolo_model: Modelo YOLO cargado\n",
    "        input_dir: Directorio con imágenes originales\n",
    "        output_dir: Directorio donde guardar las imágenes recortadas\n",
    "        valid_labels: Lista de etiquetas válidas para YOLO\n",
    "    \"\"\"\n",
    "    if invalid_labels is None:\n",
    "        invalid_labels = ['no_match']  # Ajusta según tus etiquetas\n",
    "        \n",
    "    Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Procesar todas las imágenes en el directorio\n",
    "    image_paths = list(Path(input_dir).glob('*.jpg'))\n",
    "    processed_count = 0\n",
    "    skipped_count = 0\n",
    "    \n",
    "    for img_path in tqdm(image_paths, desc=\"Preprocessing images with YOLO\"):\n",
    "        try:\n",
    "            # Detectar objetos con YOLO\n",
    "            results = yolo_model(str(img_path), verbose=False)[0]\n",
    "            \n",
    "            # Si no hay detecciones, saltar imagen\n",
    "            if len(results.boxes) == 0:\n",
    "                print(f\"\\nNo detection in {img_path.name}\")\n",
    "                skipped_count += 1\n",
    "                continue\n",
    "            \n",
    "            # Obtener la mejor detección\n",
    "            confidences = results.boxes.conf.cpu().numpy()\n",
    "            best_idx = confidences.argmax()\n",
    "            box = results.boxes[best_idx]\n",
    "            \n",
    "            cls_id = box.cls.item()\n",
    "            cls_name = results.names[int(cls_id)]\n",
    "            \n",
    "            # Verificar si la clase es válida\n",
    "            if cls_name in invalid_labels:\n",
    "                print(f\"\\nInvalid class {cls_name} in {img_path.name}\")\n",
    "                skipped_count += 1\n",
    "                continue\n",
    "            \n",
    "            # Recortar imagen\n",
    "            original_image = Image.open(img_path)\n",
    "            bbox = box.xyxy[0].cpu().numpy()\n",
    "            cropped_image = crop_image(original_image, bbox)\n",
    "            \n",
    "            # Guardar imagen recortada\n",
    "            output_path = Path(output_dir) / img_path.name\n",
    "            cropped_image.save(output_path)\n",
    "            processed_count += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\nError processing {img_path.name}: {str(e)}\")\n",
    "            skipped_count += 1\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\nPreprocessing complete:\")\n",
    "    print(f\"Processed: {processed_count} images\")\n",
    "    print(f\"Skipped: {skipped_count} images\")\n",
    "    return output_dir\n",
    "\n",
    "def crop_image(image, bbox):\n",
    "    \"\"\"Crop image using bounding box coordinates.\"\"\"\n",
    "    x1, y1, x2, y2 = [int(coord) for coord in bbox]\n",
    "    return image.crop((x1, y1, x2, y2))\n",
    "\n",
    "def plot_misclassifications(detector, data_dir, errors, is_valid=True, batch_size=32):\n",
    "    \"\"\"\n",
    "    Plot images that were misclassified by the model.\n",
    "    \n",
    "    Args:\n",
    "        detector: DNIAnomalyDetector instance\n",
    "        data_dir: Directory with images\n",
    "        errors: Array of reconstruction errors\n",
    "        is_valid: Whether these are valid images (for FP) or invalid images (for FN)\n",
    "        batch_size: Batch size for processing\n",
    "    \"\"\"\n",
    "    dataset = DNIDataset(data_dir, detector.transform)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Encontrar índices de imágenes mal clasificadas\n",
    "    if is_valid:\n",
    "        # Falsos Positivos: imágenes válidas con error > threshold\n",
    "        misclassified_idx = np.where(errors > detector.threshold)[0]\n",
    "        title = \"Falsos Positivos (Válidas clasificadas como Inválidas)\"\n",
    "    else:\n",
    "        # Falsos Negativos: imágenes inválidas con error <= threshold\n",
    "        misclassified_idx = np.where(errors <= detector.threshold)[0]\n",
    "        title = \"Falsos Negativos (Inválidas clasificadas como Válidas)\"\n",
    "    \n",
    "    if len(misclassified_idx) == 0:\n",
    "        print(f\"No hay {title}\")\n",
    "        return\n",
    "    \n",
    "    # Tomar hasta 16 imágenes para visualizar\n",
    "    n_images = min(16, len(misclassified_idx))\n",
    "    misclassified_idx = misclassified_idx[:n_images]\n",
    "    \n",
    "    fig, axes = plt.subplots(4, 4, figsize=(15, 15))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    detector.encoder.eval()\n",
    "    detector.decoder.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        current_idx = 0\n",
    "        for batch_idx, batch in enumerate(loader):\n",
    "            for img_idx in range(len(batch)):\n",
    "                global_idx = batch_idx * batch_size + img_idx\n",
    "                if global_idx in misclassified_idx:\n",
    "                    img = batch[img_idx:img_idx+1].to(detector.device)\n",
    "                    reconstructed = detector.decoder(detector.encoder(img))\n",
    "                    \n",
    "                    # Convertir a numpy para visualización\n",
    "                    original = img.cpu().squeeze(0).permute(1, 2, 0).numpy()\n",
    "                    reconstructed = reconstructed.cpu().squeeze(0).permute(1, 2, 0).numpy()\n",
    "                    \n",
    "                    # Plotear original y reconstrucción\n",
    "                    axes[current_idx*2].imshow(original)\n",
    "                    axes[current_idx*2].set_title(f'Original\\nError: {errors[global_idx]:.6f}')\n",
    "                    axes[current_idx*2].axis('off')\n",
    "                    \n",
    "                    axes[current_idx*2+1].imshow(reconstructed)\n",
    "                    axes[current_idx*2+1].set_title('Reconstrucción')\n",
    "                    axes[current_idx*2+1].axis('off')\n",
    "                    \n",
    "                    current_idx += 1\n",
    "                    if current_idx >= n_images:\n",
    "                        break\n",
    "            if current_idx >= n_images:\n",
    "                break\n",
    "    \n",
    "    plt.suptitle(title)\n",
    "    plt.tight_layout()\n",
    "    save_name = 'falsos_positivos.png' if is_valid else 'falsos_negativos.png'\n",
    "    plt.savefig(save_name)\n",
    "    plt.show()\n",
    "\n",
    "def compare_reconstruction_errors(model_path, valid_dir, invalid_dir, batch_size=32):\n",
    "    \"\"\"\n",
    "    Compare reconstruction errors between valid and invalid images.\n",
    "    \n",
    "    Args:\n",
    "        model_path: Path to the saved model\n",
    "        valid_dir: Directory with valid images\n",
    "        invalid_dir: Directory with invalid images\n",
    "        batch_size: Batch size for testing\n",
    "    \"\"\"\n",
    "    # Cargar modelo\n",
    "    detector = DNIAnomalyDetector(device='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    detector.load_model(model_path)\n",
    "    \n",
    "    detector.encoder.eval()\n",
    "    detector.decoder.eval()\n",
    "    \n",
    "    yolo_model = YOLO(\"api/best.pt\")\n",
    "    \n",
    "    print(\"Preprocesando imágenes con YOLO...\")\n",
    "    # Crear directorios temporales para las imágenes procesadas\n",
    "    valid_processed_dir = \"temp_valid_processed\"\n",
    "    invalid_processed_dir = \"temp_invalid_processed\"\n",
    "    \n",
    "    try:\n",
    "        valid_dir_processed = preprocess_dataset_with_yolo(\n",
    "            yolo_model, valid_dir, valid_processed_dir\n",
    "        )\n",
    "        invalid_dir_processed = preprocess_dataset_with_yolo(\n",
    "            yolo_model, invalid_dir, invalid_processed_dir\n",
    "        )\n",
    "        # Obtener errores para ambos conjuntos\n",
    "        valid_errors = get_reconstruction_errors(detector, valid_dir, batch_size)\n",
    "        invalid_errors = get_reconstruction_errors(detector, invalid_dir, batch_size)\n",
    "        \n",
    "        valid_above_threshold = np.sum(valid_errors > detector.threshold)\n",
    "        valid_proportion = valid_above_threshold / len(valid_errors) * 100\n",
    "        \n",
    "        invalid_below_threshold = np.sum(invalid_errors <= detector.threshold)\n",
    "        invalid_proportion = invalid_below_threshold / len(invalid_errors) * 100\n",
    "        \n",
    "        print(\"\\nMétricas de clasificación:\")\n",
    "        print(f\"Imágenes válidas por encima del threshold: {valid_above_threshold}/{len(valid_errors)} ({valid_proportion:.2f}%)\")\n",
    "        print(f\"Imágenes inválidas por debajo del threshold: {invalid_below_threshold}/{len(invalid_errors)} ({invalid_proportion:.2f}%)\")\n",
    "        \n",
    "        # Para completitud, también mostramos el complemento\n",
    "        print(f\"\\nImágenes válidas correctamente clasificadas: {len(valid_errors) - valid_above_threshold}/{len(valid_errors)} ({100 - valid_proportion:.2f}%)\")\n",
    "        print(f\"Imágenes inválidas correctamente clasificadas: {len(invalid_errors) - invalid_below_threshold}/{len(invalid_errors)} ({100 - invalid_proportion:.2f}%)\")\n",
    "        \n",
    "        # Plotear histograma comparativo\n",
    "        plt.figure(figsize=(12, 7))\n",
    "        \n",
    "        # Calcular bins comunes para ambas distribuciones\n",
    "        all_errors = np.concatenate([valid_errors, invalid_errors])\n",
    "        bins = np.linspace(min(all_errors), max(all_errors), 50)\n",
    "        \n",
    "        # Plotear ambas distribuciones\n",
    "        plt.hist(valid_errors, bins=bins, alpha=0.5, color='green', \n",
    "                 label=f'Valid Images (n={len(valid_errors)})')\n",
    "        plt.hist(invalid_errors, bins=bins, alpha=0.5, color='red',\n",
    "                 label=f'Invalid Images (n={len(invalid_errors)})')\n",
    "        \n",
    "        plt.axvline(x=detector.threshold, color='black', linestyle='--', \n",
    "                    label=f'Model Threshold: {detector.threshold:.6f}')\n",
    "        \n",
    "        plt.title('Distribution of Reconstruction Errors: Valid vs Invalid Images')\n",
    "        plt.xlabel('MSE Reconstruction Error')\n",
    "        plt.ylabel('Number of Images')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.savefig('reconstruction_errors_comparison.png')\n",
    "        plt.show()\n",
    "        \n",
    "        # Imprimir estadísticas comparativas\n",
    "        print(\"\\nEstadísticas de errores de reconstrucción:\")\n",
    "        print(\"\\nImágenes Válidas:\")\n",
    "        print(f\"Media: {valid_errors.mean():.6f}\")\n",
    "        print(f\"Mediana: {np.median(valid_errors):.6f}\")\n",
    "        print(f\"Desviación estándar: {valid_errors.std():.6f}\")\n",
    "        print(f\"Mínimo: {valid_errors.min():.6f}\")\n",
    "        print(f\"Máximo: {valid_errors.max():.6f}\")\n",
    "        \n",
    "        print(\"\\nImágenes Inválidas:\")\n",
    "        print(f\"Media: {invalid_errors.mean():.6f}\")\n",
    "        print(f\"Mediana: {np.median(invalid_errors):.6f}\")\n",
    "        print(f\"Desviación estándar: {invalid_errors.std():.6f}\")\n",
    "        print(f\"Mínimo: {invalid_errors.min():.6f}\")\n",
    "        print(f\"Máximo: {invalid_errors.max():.6f}\")\n",
    "        \n",
    "        # Calcular solapamiento entre distribuciones\n",
    "        min_overlap = max(min(valid_errors), min(invalid_errors))\n",
    "        max_overlap = min(max(valid_errors), max(invalid_errors))\n",
    "        overlap_range = max_overlap - min_overlap\n",
    "        total_range = max(max(valid_errors), max(invalid_errors)) - min(min(valid_errors), min(invalid_errors))\n",
    "        overlap_percentage = (overlap_range / total_range) * 100\n",
    "        \n",
    "        print(f\"\\nSolapamiento entre distribuciones: {overlap_percentage:.2f}%\")\n",
    "        \n",
    "        # Después de todas las métricas y el histograma, agregar:\n",
    "        print(\"\\nGenerando visualizaciones de casos mal clasificados...\")\n",
    "        plot_misclassifications(detector, valid_dir, valid_errors, is_valid=True, batch_size=batch_size)\n",
    "        plot_misclassifications(detector, invalid_dir, invalid_errors, is_valid=False, batch_size=batch_size)\n",
    "    \n",
    "    finally:\n",
    "        pass\n",
    "# Uso\n",
    "compare_reconstruction_errors(\n",
    "    model_path='dni_anomaly_detector.pt',\n",
    "    valid_dir='test/valid',\n",
    "    invalid_dir='test/invalid',\n",
    "    batch_size=32\n",
    ")"
   ],
   "id": "6f66f7ee596aec12",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "\n",
    "def test_autoencoder(model, image_path, save_path=None):\n",
    "    \"\"\"\n",
    "    Test the autoencoder by reconstructing an image and visualizing the results.\n",
    "    \n",
    "    Args:\n",
    "        model: DNIAnomalyDetector instance\n",
    "        image_path: Path to the test image\n",
    "        save_path: Optional path to save the visualization\n",
    "    \"\"\"\n",
    "    # Prepare image\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image_tensor = model.transform(image).unsqueeze(0).to(model.device)\n",
    "    \n",
    "    # Get reconstruction\n",
    "    model.encoder.eval()\n",
    "    model.decoder.eval()\n",
    "    with torch.no_grad():\n",
    "        latent = model.encoder(image_tensor)\n",
    "        reconstructed = model.decoder(latent)\n",
    "    \n",
    "    # Calculate reconstruction error\n",
    "    mse_loss = torch.nn.functional.mse_loss(reconstructed, image_tensor).item()\n",
    "    \n",
    "    # Convert tensors to images for plotting\n",
    "    original_img = image_tensor.squeeze(0).cpu().numpy().transpose(1, 2, 0)\n",
    "    reconstructed_img = reconstructed.squeeze(0).cpu().numpy().transpose(1, 2, 0)\n",
    "    \n",
    "    # Create figure\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    # Plot original\n",
    "    axes[0].imshow(original_img)\n",
    "    axes[0].set_title('Original')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # Plot reconstruction\n",
    "    axes[1].imshow(reconstructed_img)\n",
    "    axes[1].set_title('Reconstructed')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    # Plot error map\n",
    "    error_map = np.abs(original_img - reconstructed_img).mean(axis=2)\n",
    "    im = axes[2].imshow(error_map, cmap='hot')\n",
    "    axes[2].set_title(f'Error Map\\nMSE: {mse_loss:.6f}')\n",
    "    axes[2].axis('off')\n",
    "    plt.colorbar(im, ax=axes[2])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "    plt.show()\n",
    "    \n",
    "    return mse_loss\n",
    "\n",
    "def batch_test(model, test_dir, n_samples=5):\n",
    "    \"\"\"\n",
    "    Test multiple images and show their reconstructions\n",
    "    \n",
    "    Args:\n",
    "        model: DNIAnomalyDetector instance\n",
    "        test_dir: Directory containing test images\n",
    "        n_samples: Number of random samples to test\n",
    "    \"\"\"\n",
    "    from pathlib import Path\n",
    "    import random\n",
    "    \n",
    "    # Get list of images\n",
    "    image_paths = list(Path(test_dir).glob('*.jpg'))\n",
    "    \n",
    "    # Select random samples\n",
    "    if n_samples > len(image_paths):\n",
    "        n_samples = len(image_paths)\n",
    "    \n",
    "    test_images = random.sample(image_paths, n_samples)\n",
    "    \n",
    "    # Test each image\n",
    "    results = []\n",
    "    for img_path in test_images:\n",
    "        print(f\"\\nTesting {img_path.name}\")\n",
    "        mse = test_autoencoder(model, img_path)\n",
    "        confidence = model.predict(img_path)\n",
    "        results.append({\n",
    "            'image': img_path.name,\n",
    "            'mse': mse,\n",
    "            'confidence': confidence\n",
    "        })\n",
    "        print(f\"MSE: {mse:.6f}\")\n",
    "        print(f\"Confidence Score: {confidence:.6f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Cargar el modelo entrenado\n",
    "detector = DNIAnomalyDetector(device='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "detector.load_model('dni_anomaly_detector_old.pt')\n",
    "\n",
    "# Test de una sola imagen\n",
    "test_autoencoder(detector, 'test/valid/0a1e89f6-0ae2-4cab-854e-61c897cbbe13.jpg', save_path='resultado.png')\n",
    "\n",
    "# test_autoencoder(detector, 'test/invalid/DniFrente 1.jpg', save_path='resultado.png')\n",
    "\n",
    "# # Test de múltiples imágenes\n",
    "# results = batch_test(detector, 'carpeta/con/imagenes', n_samples=5)"
   ],
   "id": "a967fe7e04020347",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "detector = DNIAnomalyDetector(device='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "detector.load_model('dni_anomaly_detector_old.pt')\n",
    "\n",
    "detector.threshold"
   ],
   "id": "e4a7b2d707d37219",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
