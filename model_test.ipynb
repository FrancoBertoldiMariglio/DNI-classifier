{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Test",
   "id": "7c1f2942b2813a97"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T19:03:28.386599Z",
     "start_time": "2024-12-06T19:03:26.663Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from ultralytics import YOLO\n",
    "from torch.utils.data import DataLoader\n",
    "from autoencoder import DNIAnomalyDetector, DNIDataset\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "def preprocess_images_with_yolo(yolo_model, input_dir, output_dir, global_output_dir=None, invalid_labels=None):\n",
    "    \"\"\"\n",
    "    Detect, crop, and ensure horizontal orientation of DNI images using YOLO.\n",
    "\n",
    "    Args:\n",
    "        yolo_model: YOLO model for detection.\n",
    "        input_dir: Directory containing input images.\n",
    "        output_dir: Directory to save processed images for this input_dir.\n",
    "        global_output_dir: Directory to save all cropped images globally.\n",
    "        invalid_labels: List of labels to ignore during processing.\n",
    "    \"\"\"\n",
    "    invalid_labels = invalid_labels or ['no_match']\n",
    "    Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "    if global_output_dir:\n",
    "        Path(global_output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    image_paths = list(Path(input_dir).glob('*.jpg'))\n",
    "    for img_path in tqdm(image_paths, desc=f\"Processing {input_dir}\"):\n",
    "        try:\n",
    "            results = yolo_model(str(img_path), verbose=False)[0]\n",
    "            if not results.boxes:  # Skip if no detection\n",
    "                continue\n",
    "\n",
    "            # Get the best detection\n",
    "            best_box = results.boxes[results.boxes.conf.argmax()]\n",
    "            cls_name = results.names[int(best_box.cls.item())]\n",
    "            if cls_name in invalid_labels:\n",
    "                continue\n",
    "\n",
    "            # Crop and reorient image\n",
    "            bbox = best_box.xyxy[0].cpu().numpy()\n",
    "            cropped_image = crop_and_reorient_image(Image.open(img_path), bbox)\n",
    "\n",
    "            # Save to output directories\n",
    "            cropped_image.save(Path(output_dir) / img_path.name)\n",
    "            if global_output_dir:\n",
    "                cropped_image.save(Path(global_output_dir) / img_path.name)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {img_path.name}: {e}\")\n",
    "\n",
    "def crop_and_reorient_image(image, bbox):\n",
    "    \"\"\"\n",
    "    Crop and ensure the image is horizontal (width > height).\n",
    "\n",
    "    Args:\n",
    "        image: PIL Image to process.\n",
    "        bbox: Bounding box [x1, y1, x2, y2].\n",
    "\n",
    "    Returns:\n",
    "        A cropped and correctly oriented PIL Image.\n",
    "    \"\"\"\n",
    "    x1, y1, x2, y2 = map(int, bbox)\n",
    "    cropped_image = image.crop((x1, y1, x2, y2))\n",
    "\n",
    "    # Ensure horizontal orientation\n",
    "    if cropped_image.height > cropped_image.width:\n",
    "        cropped_image = cropped_image.rotate(90, expand=True)\n",
    "    \n",
    "    return cropped_image\n",
    "\n",
    "def crop_image(image, bbox):\n",
    "    \"\"\"Crop image using bounding box coordinates.\"\"\"\n",
    "    x1, y1, x2, y2 = map(int, bbox)\n",
    "    return image.crop((x1, y1, x2, y2))\n",
    "\n",
    "def compute_reconstruction_stats(detector, data_dir, batch_size=32):\n",
    "    dataset = DNIDataset(data_dir, detector.transform)\n",
    "    confidences = []\n",
    "    errors = []\n",
    "    \n",
    "    for img_path in tqdm(dataset.image_paths):\n",
    "        confidence, error = detector.predict(img_path)\n",
    "        confidences.append(confidence)\n",
    "        errors.append(error)\n",
    "        \n",
    "    return np.array(confidences), np.array(errors)\n",
    "\n",
    "def plot_distributions(valid_conf, invalid_conf, valid_err, invalid_err, threshold, title):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Confidence plot\n",
    "    ax1.hist(valid_conf, bins=50, alpha=0.5, label='Valid', color='green')\n",
    "    ax1.hist(invalid_conf, bins=50, alpha=0.5, label='Invalid', color='red')\n",
    "    ax1.set_title(f'{title} - Confidence Scores')\n",
    "    ax1.set_xlabel('Confidence Score')\n",
    "    ax1.set_ylabel('Frequency')\n",
    "    ax1.legend()\n",
    "    ax1.grid(alpha=0.3)\n",
    "    \n",
    "    # Error plot\n",
    "    ax2.hist(valid_err, bins=50, alpha=0.5, label='Valid', color='green')\n",
    "    ax2.hist(invalid_err, bins=50, alpha=0.5, label='Invalid', color='red')\n",
    "    ax2.axvline(x=threshold, color='black', linestyle='--', label='Threshold')\n",
    "    ax2.set_title(f'{title} - Reconstruction Errors')\n",
    "    ax2.set_xlabel('Reconstruction Error')\n",
    "    ax2.set_ylabel('Frequency')\n",
    "    ax2.legend()\n",
    "    ax2.grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def print_model_statistics(model_name, valid_conf, invalid_conf, valid_err, invalid_err, threshold):\n",
    "    print(f\"\\n{model_name} Model Statistics:\")\n",
    "    print(f\"Valid - Mean Confidence: {valid_conf.mean():.3f}\")\n",
    "    print(f\"Invalid - Mean Confidence: {invalid_conf.mean():.3f}\")\n",
    "    print(f\"Valid - Mean Error: {valid_err.mean():.3f}\")\n",
    "    print(f\"Invalid - Mean Error: {invalid_err.mean():.3f}\")\n",
    "    print(f\"Threshold: {threshold:.3f}\")\n",
    "\n",
    "def process_and_evaluate(models_info, test_dirs):\n",
    "    for model_type, info in models_info.items():\n",
    "        model = DNIAnomalyDetector(device='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        model.load_model(info['model_path'])\n",
    "        \n",
    "        valid_conf, valid_err = compute_reconstruction_stats(model, test_dirs[f'valid{model_type}'])\n",
    "        invalid_conf, invalid_err = compute_reconstruction_stats(model, test_dirs[f'invalid{model_type}'])\n",
    "        \n",
    "        plot_distributions(valid_conf, invalid_conf, valid_err, invalid_err, \n",
    "                         model.threshold, f\"{model_type} Model\")\n",
    "        print_model_statistics(model_type, valid_conf, invalid_conf, valid_err, \n",
    "                             invalid_err, model.threshold)\n",
    "# Usage\n",
    "process_and_evaluate(\n",
    "    models_info={\n",
    "        'Frente': 'models/dni_anomaly_detector_frente.pt',\n",
    "        'Dorso': 'models/dni_anomaly_detector_dorso.pt'\n",
    "    },\n",
    "    test_dirs={\n",
    "        'validFrente': 'test/validFrente',\n",
    "        'invalidFrente': 'test/invalidFrente',\n",
    "        'validDorso': 'test/validDorso',\n",
    "        'invalidDorso': 'test/invalidDorso'\n",
    "    }\n",
    ")"
   ],
   "id": "6f66f7ee596aec12",
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 135\u001B[0m\n\u001B[1;32m    132\u001B[0m         print_model_statistics(model_type, valid_conf, invalid_conf, valid_err, \n\u001B[1;32m    133\u001B[0m                              invalid_err, model\u001B[38;5;241m.\u001B[39mthreshold)\n\u001B[1;32m    134\u001B[0m \u001B[38;5;66;03m# Usage\u001B[39;00m\n\u001B[0;32m--> 135\u001B[0m \u001B[43mprocess_and_evaluate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    136\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodels_info\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m{\u001B[49m\n\u001B[1;32m    137\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mFrente\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mmodels/dni_anomaly_detector_frente.pt\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    138\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mDorso\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mmodels/dni_anomaly_detector_dorso.pt\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\n\u001B[1;32m    139\u001B[0m \u001B[43m    \u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    140\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtest_dirs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m{\u001B[49m\n\u001B[1;32m    141\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mvalidFrente\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtest/validFrente\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    142\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43minvalidFrente\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtest/invalidFrente\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    143\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mvalidDorso\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtest/validDorso\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    144\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43minvalidDorso\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtest/invalidDorso\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\n\u001B[1;32m    145\u001B[0m \u001B[43m    \u001B[49m\u001B[43m}\u001B[49m\n\u001B[1;32m    146\u001B[0m \u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[1], line 125\u001B[0m, in \u001B[0;36mprocess_and_evaluate\u001B[0;34m(models_info, test_dirs)\u001B[0m\n\u001B[1;32m    123\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m model_type, info \u001B[38;5;129;01min\u001B[39;00m models_info\u001B[38;5;241m.\u001B[39mitems():\n\u001B[1;32m    124\u001B[0m     model \u001B[38;5;241m=\u001B[39m DNIAnomalyDetector(device\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39mis_available() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m--> 125\u001B[0m     model\u001B[38;5;241m.\u001B[39mload_model(\u001B[43minfo\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mmodel_path\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m)\n\u001B[1;32m    127\u001B[0m     valid_conf, valid_err \u001B[38;5;241m=\u001B[39m compute_reconstruction_stats(model, test_dirs[\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mvalid\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodel_type\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m])\n\u001B[1;32m    128\u001B[0m     invalid_conf, invalid_err \u001B[38;5;241m=\u001B[39m compute_reconstruction_stats(model, test_dirs[\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124minvalid\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodel_type\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m])\n",
      "\u001B[0;31mTypeError\u001B[0m: string indices must be integers"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "\n",
    "def test_autoencoder(model, image_path, save_path=None):\n",
    "    \"\"\"\n",
    "    Test the autoencoder by reconstructing an image and visualizing the results.\n",
    "    \n",
    "    Args:\n",
    "        model: DNIAnomalyDetector instance\n",
    "        image_path: Path to the test image\n",
    "        save_path: Optional path to save the visualization\n",
    "    \"\"\"\n",
    "    # Prepare image\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image_tensor = model.transform(image).unsqueeze(0).to(model.device)\n",
    "    \n",
    "    # Get reconstruction\n",
    "    model.encoder.eval()\n",
    "    model.decoder.eval()\n",
    "    with torch.no_grad():\n",
    "        latent = model.encoder(image_tensor)\n",
    "        reconstructed = model.decoder(latent)\n",
    "    \n",
    "    # Calculate reconstruction error\n",
    "    mse_loss = torch.nn.functional.mse_loss(reconstructed, image_tensor).item()\n",
    "    \n",
    "    # Convert tensors to images for plotting\n",
    "    original_img = image_tensor.squeeze(0).cpu().numpy().transpose(1, 2, 0)\n",
    "    reconstructed_img = reconstructed.squeeze(0).cpu().numpy().transpose(1, 2, 0)\n",
    "    \n",
    "    # Create figure\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    # Plot original\n",
    "    axes[0].imshow(original_img)\n",
    "    axes[0].set_title('Original')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # Plot reconstruction\n",
    "    axes[1].imshow(reconstructed_img)\n",
    "    axes[1].set_title('Reconstructed')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    # Plot error map\n",
    "    error_map = np.abs(original_img - reconstructed_img).mean(axis=2)\n",
    "    im = axes[2].imshow(error_map, cmap='hot')\n",
    "    axes[2].set_title(f'Error Map\\nMSE: {mse_loss:.6f}')\n",
    "    axes[2].axis('off')\n",
    "    plt.colorbar(im, ax=axes[2])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "    plt.show()\n",
    "    \n",
    "    return mse_loss\n",
    "\n",
    "# Cargar el modelo entrenado\n",
    "detector = DNIAnomalyDetector(device='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "detector.load_model('dni_anomaly_detector_old.pt')\n",
    "\n",
    "# Test de una sola imagen\n",
    "test_autoencoder(detector, 'test/valid/0a1e89f6-0ae2-4cab-854e-61c897cbbe13.jpg', save_path='resultado.png')\n",
    "\n",
    "# test_autoencoder(detector, 'test/invalid/DniFrente 1.jpg', save_path='resultado.png')\n",
    "\n",
    "# # Test de múltiples imágenes\n",
    "# results = batch_test(detector, 'carpeta/con/imagenes', n_samples=5)"
   ],
   "id": "a967fe7e04020347",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "detector = DNIAnomalyDetector(device='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "detector.load_model('dni_anomaly_detector_old.pt')\n",
    "\n",
    "detector.threshold"
   ],
   "id": "e4a7b2d707d37219",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
